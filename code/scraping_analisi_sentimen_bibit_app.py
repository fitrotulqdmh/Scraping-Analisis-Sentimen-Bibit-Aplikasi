# -*- coding: utf-8 -*-
"""Scraping_Analisi Sentimen_bibit app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gX8GEjZFcCry2UO7yjYbDHGMntfzVcjO
"""

!pip install google-play-scraper

from google_play_scraper import app
import pandas as pd
import numpy as np

"""# **Scraping Data**"""

from google_play_scraper import Sort, reviews

result, continuation_token = reviews(
  'com.bibit.bibitid',
  lang='id',
  country='id',
  sort=Sort.MOST_RELEVANT,
  count=200,
  filter_score_with=None)

df_baru = pd.DataFrame(np.array(result),columns=['review'])
df_baru = df_baru.join(pd.DataFrame(df_baru.pop('review').tolist()))
df_baru.head()

len(df_baru.index)

df_baru[['score', 'content']].head()

new_df = df_baru[['userName', 'score', 'at', 'content']]
new_df.head()

new_df.to_csv("bibit_scraping_data_new.csv", index = False)

"""## Pelabelan"""

def labelling(score):
  if score < 3:
    return 'Negatif'
  elif score == 4  :
    return 'Positif'
  elif score == 5 :
    return 'Positif'
new_df['Label'] = new_df ['score'].apply(labelling)
new_df.head(200)

new_df = new_df[['content', 'Label']]

new_df.head()

"""## **Data Cleaning**"""

new_df.info()

new_df.isna()

new_df.isna().any()

new_df.isnull().sum()

new_df.dropna(subset=['Label'],inplace = True)

new_df.isnull().sum()

new_df.head(10)

new_df.to_csv("bibit_scraping_data_new.csv", index = False)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import tensorflow as tf

from numpy import array
from sklearn.model_selection import train_test_split

from tensorflow.keras import regularizers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

data = pd.read_csv('/content/bibit_scraping_data_new.csv')
data.head(10)

#CONVERT LABEL

y = [ 0 if i=='Negatif' else 1 for i in data['Label']]
x = data['content']
print(x[:5], y[:5])

#Split data

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

(unique, counts) = np.unique(y_test, return_counts=True)
frequencies = np.asarray((unique, counts)).T
print(frequencies)

"""# Model Configuration"""

vocab_size = 200000
embedding_dim = 16
max_length = 20
trunch_type = 'post'
padding_type = 'post'
oov_tok = "<OOV>"

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(x)

sekuens_train = tokenizer.texts_to_sequences(X_train)
sekuens_test = tokenizer.texts_to_sequences(X_test)

padded_train = pad_sequences(sekuens_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)
padded_test = pad_sequences(sekuens_test, maxlen=max_length, padding=padding_type, truncating=trunch_type)

padded_train = np.array(padded_train)
label_train = np.array(y_train)

padded_test = np.array(padded_test)
label_test = np.array(y_test)

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

history = model.fit(padded_train, label_train, epochs=20,
                    batch_size=32, validation_data=(padded_test, label_test),
                    steps_per_epoch=15, verbose=2)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_lodd = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_acc, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""# Confusion Matrix"""

y_pred = np.rint(model.predict(padded_test))
y_pred

cm = tf.math.confusion_matrix(labels=label_test, predictions=y_pred)
cm

import seaborn as sns
figure = plt.figure(figsize=(8, 8))
ax = plt.subplot()
data = np.asarray(cm).reshape(2, 2)

sns.heatmap(data, annot=True, cmap=plt.cm.Blues, ax=ax)
plt.tight_layout()
plt.ylabel('True Label')
plt.xlabel('Predict Label')
plt.show()

"""# Pengujian Sentimen Menggunakan Model"""

sentence = ['bibit sangat membantu dalam menabung']
sequences = tokenizer.texts_to_sequences(sentence)
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
print(np.rint(model.predict(padded)))